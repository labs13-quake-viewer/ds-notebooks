{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from USGS\n",
    "\n",
    "https://earthquake.usgs.gov/fdsnws/event/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium import plugins\n",
    "from io import StringIO\n",
    "import requests\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'format': 'csv', \n",
    "#     'starttime': None,  # default last 30 days\n",
    "#     'endtime': '2019-06-03',  # default now\n",
    "    'minmagnitude': 2.5,  # default null\n",
    "    'limit': None,  # default null, returns 404 over 20,000\n",
    "}\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "r = requests.get(url, params=payload)\n",
    "\n",
    "df = pd.read_csv(StringIO(r.text))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Animated Map with `folium`\n",
    "\n",
    "https://nbviewer.jupyter.org/github/python-visualization/folium/blob/master/examples/Plugins.ipynb#Timestamped-GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get faults\n",
    "r = requests.get('https://raw.githubusercontent.com/'\n",
    "                 'fraxen/tectonicplates/master/GeoJSON/'\n",
    "                 'PB2002_boundaries.json')\n",
    "\n",
    "fault_features = r.json()['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    {\n",
    "        'type': 'Feature',\n",
    "        'geometry': {\n",
    "            'type': 'Point',\n",
    "            'coordinates': [r['longitude'], r['latitude']],\n",
    "        },\n",
    "        'properties': {\n",
    "            'time': r['time'][0:-1],\n",
    "            'popup': (\n",
    "                f\"<strong>Time:</strong> {r['time']}<br>\"\n",
    "                f\"<strong>Place:</strong> {r['place']}<br>\"\n",
    "                f\"<strong>Magnitude:</strong> {r['mag']} {r['magType']}<br>\"\n",
    "                f\"<strong>Depth:</strong> {r['depth']}<br>\"\n",
    "            ),\n",
    "            'icon': 'circle',\n",
    "            'iconstyle': {\n",
    "                'fillOpacity': 0.5,\n",
    "                'stroke': 0,\n",
    "                'radius': r['mag'] * 2.5\n",
    "            },\n",
    "        }\n",
    "    } for i, r in df.iterrows()\n",
    "]\n",
    "\n",
    "m = folium.Map(\n",
    "#     location=()\n",
    "    tiles='CartoDBpositron',\n",
    "#     zoom_start=1,\n",
    "#     no_wrap=True,\n",
    "    min_zoom=1.5,\n",
    "    max_zoom=5,\n",
    "    world_copy_jump=True,\n",
    ")\n",
    "\n",
    "# add faults\n",
    "folium.GeoJson(\n",
    "    {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': fault_features,\n",
    "    },\n",
    "    style_function = lambda x: {\n",
    "        'color': 'red',\n",
    "        'weight': 0.5,\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "plugins.TimestampedGeoJson(\n",
    "    {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    },\n",
    "    period='PT6H', # six hour\n",
    "    time_slider_drag_update=True,\n",
    "    duration='PT12H',\n",
    "    date_options='YYYY-MM-DD HH UTC'\n",
    ").add_to(m)\n",
    "\n",
    "folium.plugins.Fullscreen(\n",
    "    position='topright',\n",
    "    force_separate_button=True,\n",
    ").add_to(m)\n",
    "\n",
    "# m.save('earthquakes.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damage Data\n",
    "from https://www.ngdc.noaa.gov/nndc/struts/form?t=101650&s=1&d=1\n",
    "\n",
    "> The Significant Earthquake Database contains information on destructive earthquakes from 2150 B.C. to the present that meet at least one of the following criteria: \n",
    "> * Moderate damage (approximately $1 million or more)\n",
    "> * 10 or more deaths\n",
    "> * Magnitude 7.5 or greater\n",
    "> * Modified Mercalli Intensity X or greater\n",
    "> * the earthquake generated a tsunami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmg = pd.read_csv('https://www.ngdc.noaa.gov/nndc/struts/results?'\n",
    "                  'type_0=Exact&query_0=$ID&t=101650&s=13&d=189&dfn=signif.txt',\n",
    "                  sep='\\t')\n",
    "\n",
    "dmg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour Plot\n",
    "\n",
    "One nice way to display likelihoods of earthquake, once we have them\n",
    "\n",
    "https://www.tjansson.dk/2018/10/contour-map-in-folium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "import branca\n",
    "from folium import plugins\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import geojsoncontour\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    " \n",
    "# Setup\n",
    "temp_mean = 12\n",
    "temp_std  = 2\n",
    "debug     = False\n",
    " \n",
    "# Setup colormap\n",
    "colors = ['#d7191c',  '#fdae61',  '#ffffbf',  '#abdda4',  '#2b83ba']\n",
    "vmin   = temp_mean - 2 * temp_std\n",
    "vmax   = temp_mean + 2 * temp_std\n",
    "levels = len(colors)\n",
    "cm     = branca.colormap.LinearColormap(colors, vmin=vmin, vmax=vmax).to_step(levels)\n",
    " \n",
    "# Create a dataframe with fake data\n",
    "df = pd.DataFrame({\n",
    "    'longitude':   np.random.normal(11.84,     0.15,     1000),\n",
    "    'latitude':    np.random.normal(55.55,     0.15,     1000),\n",
    "    'temperature': np.random.normal(temp_mean, temp_std, 1000)})\n",
    " \n",
    "# The original data\n",
    "x_orig = np.asarray(df.longitude.tolist())\n",
    "y_orig = np.asarray(df.latitude.tolist())\n",
    "z_orig = np.asarray(df.temperature.tolist())\n",
    " \n",
    "# Make a grid\n",
    "x_arr          = np.linspace(np.min(x_orig), np.max(x_orig), 500)\n",
    "y_arr          = np.linspace(np.min(y_orig), np.max(y_orig), 500)\n",
    "x_mesh, y_mesh = np.meshgrid(x_arr, y_arr)\n",
    " \n",
    "# Grid the values\n",
    "z_mesh = griddata((x_orig, y_orig), z_orig, (x_mesh, y_mesh), method='linear')\n",
    " \n",
    "# Gaussian filter the grid to make it smoother\n",
    "sigma = [5, 5]\n",
    "z_mesh = sp.ndimage.filters.gaussian_filter(z_mesh, sigma, mode='constant')\n",
    " \n",
    "# Create the contour\n",
    "contourf = plt.contourf(x_mesh, y_mesh, z_mesh, levels, alpha=0.5, colors=colors, linestyles='None', vmin=vmin, vmax=vmax)\n",
    " \n",
    "# Convert matplotlib contourf to geojson\n",
    "geojson = geojsoncontour.contourf_to_geojson(\n",
    "    contourf=contourf,\n",
    "    min_angle_deg=3.0,\n",
    "    ndigits=5,\n",
    "    stroke_width=1,\n",
    "    fill_opacity=0.5)\n",
    " \n",
    "# Set up the folium plot\n",
    "geomap = folium.Map([df.latitude.mean(), df.longitude.mean()], zoom_start=10, tiles=\"cartodbpositron\")\n",
    " \n",
    "# Plot the contour plot on folium\n",
    "folium.GeoJson(\n",
    "    geojson,\n",
    "    style_function=lambda x: {\n",
    "        'color':     x['properties']['stroke'],\n",
    "        'weight':    x['properties']['stroke-width'],\n",
    "        'fillColor': x['properties']['fill'],\n",
    "        'opacity':   0.6,\n",
    "    }).add_to(geomap)\n",
    " \n",
    "# Add the colormap to the folium map\n",
    "cm.caption = 'Temperature'\n",
    "geomap.add_child(cm)\n",
    " \n",
    "# Fullscreen mode\n",
    "plugins.Fullscreen(position='topright', force_separate_button=True).add_to(geomap)\n",
    " \n",
    "# Plot the data\n",
    "# geomap.save(f'data/folium_contour_temperature_map.html'\n",
    "geomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively Download USGS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "def dl_quake_data(start_date, end_date, page_limt=10000):\n",
    "    start_date = parse(start_date).isoformat()\n",
    "    end_date = parse(end_date).isoformat()\n",
    "    payload = {\n",
    "        'format': 'csv',\n",
    "        'starttime': start_date,\n",
    "        'endtime': end_date,\n",
    "        'minmagnitude': 2,\n",
    "        'limit': page_limt,\n",
    "        'orderby': 'time-asc',\n",
    "    }\n",
    "    url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "    r = requests.get(url, params=payload)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        print('Error', r.status_code, r.url)\n",
    "        return False\n",
    "    \n",
    "    df = pd.read_csv(StringIO(r.text))\n",
    "    \n",
    "    dt_min = df['time'].iloc[0]\n",
    "    dt_max = df['time'].iloc[-1]\n",
    "    \n",
    "    fn = (f'{parse(dt_min).strftime(\"%Y-%m-%d\")}_'\n",
    "          f'{parse(dt_max).strftime(\"%Y-%m-%d\")}')\n",
    "    df.to_csv(f'data/{fn}.csv', index=False)\n",
    "    \n",
    "    print(fn)\n",
    "    \n",
    "    if len(df) == page_limt:\n",
    "         dl_quake_data(start_date=dt_max,\n",
    "                       end_date=end_date)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done '1999-01-01' to '2019-01-01'\n",
    "# dl_quake_data('1999-01-01', '2009-01-01', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dfs = []\n",
    "for csv in Path('data').iterdir():\n",
    "    dfs.append(pd.read_csv(csv))\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['updated'] = pd.to_datetime(df['updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['mag'].hist(bins=int((df.mag.max() - df.mag.min()) * 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(df['updated'] - df['time']).dt.days.hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['time'].dt.year)['id'].count().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Earthquake Events to Region\n",
    "\n",
    "This is done because different regions have different detective power for small magnitude eathquakes. Here I generate a grid of equi-distant points around the globe before clustering earthquake events to the nearest node.\n",
    "\n",
    "As improvements, I could define these regions algorithmically (like with [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html), [clusterpy](https://github.com/clusterpy/), [Moran'sI](https://en.wikipedia.org/wiki/Moran's_I)) based on earthquake characteristics in each regions. It may be that a special model already exists for this purpose [in extant research](https://www.researchgate.net/publication/260702383_A_detailed_seismic_zonation_model_for_shallow_earthquakes_in_the_broader_Aegean_area). Alternatively, knowing where seismic stations are located and how sensitive they are could help inform region definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "downsamp = df[['latitude', 'longitude']].sample(frac=0.125)\n",
    "clusterer = hdbscan.HDBSCAN(metric='haversine')\n",
    "clusterer.fit(downsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roughly equidistant points on a unit-radius cartesian sphere\n",
    "#https://stackoverflow.com/a/44164075/11208892\n",
    "import numpy as np\n",
    "import mpl_toolkits.mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_pts = 3000\n",
    "indices = np.arange(0, num_pts, dtype=float) + 0.5\n",
    "\n",
    "phi = np.arccos(1 - 2*indices/num_pts)\n",
    "theta = np.pi * (1 + 5**0.5) * indices\n",
    "\n",
    "x = np.cos(theta) * np.sin(phi)\n",
    "y = np.sin(theta) * np.sin(phi)\n",
    "z = np.cos(phi)\n",
    "\n",
    "plt.figure().add_subplot(111, projection='3d').scatter(x, y, z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lat/lon, ignoring that the earth is not a sphere\n",
    "# https://stackoverflow.com/questions/1185408/converting-from-longitude-latitude-to-cartesian-coordinates\n",
    "clust_df = pd.DataFrame({'clust_lat': 180 * np.arcsin(z / 1) / np.pi, \n",
    "                         'clust_lon': 180 * np.arctan2(y, x) / np.pi})\n",
    "clust_df['clust_id'] = clust_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Quake Events to Cluster Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "dist = DistanceMetric.get_metric('haversine') # requires radian inputs\n",
    "\n",
    "clust_rads = clust_df[['clust_lat', 'clust_lon']].values * np.pi / 180\n",
    "quake_rads = df[['latitude', 'longitude']].values * np.pi / 180\n",
    "\n",
    "nearest_centers = []\n",
    "step = 10000\n",
    "i = 0\n",
    "while i < len(quake_rads):\n",
    "    print(round(i/len(quake_rads))*100, '%')\n",
    "    dists = dist.pairwise(quake_rads[i:i+step], clust_rads)\n",
    "    indices = np.argmin(dists, axis=1)  # indices of nearest cluster\n",
    "    nearest_centers.append(clust_rads[indices])\n",
    "    i += step\n",
    "    \n",
    "nearest_centers = np.vstack(nearest_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clust_lat'], df['clust_lon'] = np.nan, np.nan\n",
    "df[['clust_lat', 'clust_lon']] = 180 * nearest_centers / np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(clust_df, on=['clust_lat', 'clust_lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to clusters with >= 2000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clust_cnts = df.groupby(['clust_id']).size()\n",
    "clust_cnts = clust_cnts.loc[clust_cnts >= 2000]\n",
    "\n",
    "clust_df = clust_df.loc[clust_df['clust_id'].isin(clust_cnts.index)]\n",
    "df = df.loc[df['clust_id'].isin(clust_cnts.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_cnts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Cluster Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import folium.plugins\n",
    "\n",
    "m = folium.Map(tiles='CartoDBpositron')\n",
    "clust_cnts = df.groupby(['clust_id', 'clust_lat', 'clust_lon']).size()\n",
    "\n",
    "for i, v in clust_cnts.items():\n",
    "    folium.CircleMarker(\n",
    "        i[1:], \n",
    "        radius=np.sqrt((v/100) / np.pi) * 2,\n",
    "        fill=True,\n",
    "        fill_opacity=0.5,\n",
    "        stroke=0,\n",
    "        popup=f\"id:{i[0]}<br>events: {v}\",\n",
    "    ).add_to(m)\n",
    "\n",
    "folium.plugins.Fullscreen(\n",
    "    position='topright',\n",
    "    force_separate_button=True,\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Cut-Off Magnitude for Each Node, `M_c`\n",
    "\n",
    "Here I compare OLS and Mode methods. For a more robust method, see [Bayesian method](https://medium.com/the-history-risk-forecast-of-perils/exploring-the-fascinating-world-of-incomplete-seismicity-data-part-i-ii-bayesian-inference-386338b43b71)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def OLS_mc_a_b(node_df):\n",
    "    node_df = node_df.copy()\n",
    "    node_df['mag'] = node_df['mag'].round(1)\n",
    "    node_df = node_df.loc[node_df['mag']<6]\n",
    "    \n",
    "    n_steps = int((node_df['mag'].max() - node_df['mag'].min()) * 10) + 1\n",
    "    mag_range = np.linspace(\n",
    "        node_df['mag'].min(), \n",
    "        node_df['mag'].max(), \n",
    "        n_steps)\n",
    "    mag_range = [round(x, 2) for x in mag_range]\n",
    "\n",
    "    counts = pd.Series(\n",
    "        index=mag_range,\n",
    "        data=[sum(node_df['mag'] >= x) for x in mag_range]\n",
    "    )\n",
    "\n",
    "    best_score = 0\n",
    "    best_m_c, a, b = [None] * 3\n",
    "\n",
    "    for M_c in np.arange(node_df['mag'].min(), 4.6, 0.1):\n",
    "        M_c = round(M_c, 2)\n",
    "        data = counts.loc[counts.index>=M_c]\n",
    "        X = np.array(data.index).reshape(-1, 1)\n",
    "        y = np.log10(data.values)\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        score = reg.score(X, y)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_m_c = M_c\n",
    "            best_score = score\n",
    "            a = reg.intercept_\n",
    "            b = -1 * reg.coef_[0]\n",
    "\n",
    "    return best_m_c, a, b\n",
    "\n",
    "\n",
    "def mode_m_c_a_b(node_df):\n",
    "    node_df = node_df.copy()\n",
    "    node_df['mag'] = node_df['mag'].round(1)\n",
    "    node_df = node_df.loc[node_df['mag']<6]\n",
    "    m_c = node_df['mag'].mode()[0]\n",
    "    node_df = node_df.loc[node_df['mag']>=m_c]\n",
    "    a, b, _ = fit_GR_OLS(node_df)\n",
    "    \n",
    "    return m_c, a, b\n",
    "\n",
    "\n",
    "def fit_GR_OLS(node_df):\n",
    "    n_steps = int((node_df['mag'].max() - node_df['mag'].min()) * 10) + 1\n",
    "    mag_range = np.linspace(\n",
    "        node_df['mag'].min(), \n",
    "        node_df['mag'].max(), \n",
    "        n_steps)\n",
    "    mag_range = [round(x, 2) for x in mag_range]\n",
    "\n",
    "    counts = pd.Series(\n",
    "        index=mag_range,\n",
    "        data=[sum(node_df['mag'] >= x) for x in mag_range]\n",
    "    )\n",
    "\n",
    "    X = np.array(counts.index).reshape(-1, 1)\n",
    "    y = np.log10(counts.values)\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    a = reg.intercept_\n",
    "    b = -1 * reg.coef_[0]\n",
    "    mse = mean_squared_error(y, reg.predict(X))\n",
    "    \n",
    "    return a, b, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_id = 609\n",
    "node_df = df.loc[df['clust_id']==clust_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OLS method\n",
    "m_c, a, b = OLS_mc_a_b(node_df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "node_df['mag'].hist(bins=50, ax=ax)\n",
    "plt.axvline(x=m_c, linewidth=2, color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode method\n",
    "m_c, a, b = mode_m_c_a_b(node_df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "node_df['mag'].hist(bins=50, ax=ax)\n",
    "plt.axvline(x=m_c, linewidth=2, color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mode method\n",
    "clust_df['m_c'], clust_df['a'], clust_df['b'] = np.nan, np.nan, np.nan\n",
    "\n",
    "for clust_id in df['clust_id'].unique():\n",
    "    node_df = df.loc[df['clust_id']==clust_id]\n",
    "    m_c, a, b = mode_m_c_a_b(node_df)\n",
    "    clust_df.loc[clust_df['clust_id']==clust_id, ['m_c', 'a', 'b']] = m_c, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_df['m_c'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Earthquake events by `m_c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(clust_df[['clust_id', 'm_c', 'a', 'b']], on='clust_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"mag >= m_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Many Samples?\n",
    "\n",
    "In literature, 50 of the most recent quakes were analyzed when the magnitude cutoff was 4.0. However, some regions have a lower threshold, allowing for exponentially more events. Therefore, I will take exponentially more events when calculated features for those regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_samples(m_c):\n",
    "    # From research:\n",
    "    #  m_c=4, y=50\n",
    "    # Extrapolated to the mc=0 case, with exponential increase\n",
    "    #  m_c=0, y=500000\n",
    "    # log10(n_events) = slope * m_c + y_int\n",
    "    y_int = np.log10(500000)\n",
    "    slope = -1 # by definition of logarithm\n",
    "\n",
    "    return int(round(10 ** (slope * m_c + y_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slolam, solstice, teksystems global services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at weekly interval, collect past 100 events, predict whether 5.5+ will occur in the next 30 days\n",
    "\n",
    "strt_sunday = df['time'].min() - pd.to_timedelta(df['time'].min().dayofweek, unit='d')\n",
    "end_sunday = (df['time'].max() - pd.to_timedelta(df['time'].max().dayofweek, unit='d') - \n",
    "              pd.to_timedelta(30, unit='d')) # leave 30 day buffer at end to assess whether quake occured\n",
    "sampling_periods = pd.date_range(strt_sunday, end_sunday, freq='W')\n",
    "\n",
    "x_df = pd.DataFrame(columns=['clust_id','sample_date',\n",
    "                             'T_days','M_mean','dEsq',\n",
    "                             'a_lsq','b_lsq','a_mlk', 'b_mlk','mse',\n",
    "                             'diff_M_max_obs_exp','mu','c',\n",
    "                             'quake_next30'])\n",
    "\n",
    "for node_id, node_alltime_df in df.loc[df['clust_id']==378].groupby(['clust_id']):\n",
    "    print(node_id)\n",
    "    \n",
    "    m_c = clust_df.loc[clust_df['clust_id']==node_id, 'm_c'].values[0]\n",
    "    N_events = get_n_samples(m_c)\n",
    "    \n",
    "    for sample_date in sampling_periods:\n",
    "        print(sample_date) #2007-01-14 02:03:26.520000+00:00\n",
    "        # check if 5.5+ mag quake happend in next 30 days\n",
    "        condition = ((sample_date < node_alltime_df['time']) & \n",
    "                     (node_alltime_df['time'] < (sample_date + pd.to_timedelta(30, unit='d'))))\n",
    "        quake_next30 = sum(node_alltime_df.loc[condition, 'mag'] >= 5.5) > 0\n",
    "        \n",
    "        # take last 100 events, compute features\n",
    "        node_df = node_alltime_df.loc[node_alltime_df['time']<=sample_date]\n",
    "        \n",
    "        if len(node_df) < N_events:\n",
    "            continue\n",
    "        \n",
    "        node_df = node_df[-N_events:]\n",
    "\n",
    "        # T: time period in days\n",
    "        T = (node_df['time'].max() - node_df['time'].min()).days\n",
    "\n",
    "        # M_mean: mean Magnitude\n",
    "        M_mean = node_df['mag'].mean()\n",
    "\n",
    "        # dEsq: seismic energy release\n",
    "        dEsq = np.sum(\n",
    "            np.sqrt(\n",
    "                np.power(\n",
    "                    np.array([10]*len(node_df)), \n",
    "                    11.8+1.5*node_df['mag']\n",
    "                )\n",
    "            )\n",
    "        ) / T\n",
    "\n",
    "        # a and b values (2 methods) and MSE from GR law\n",
    "        n = len(node_df)\n",
    "        # some issues in here:\n",
    "        a_lsq, b_lsq, mse = fit_GR_OLS(node_df)\n",
    "        \n",
    "        b_mlk = np.log10(np.e) / (node_df['mag'].mean() - node_df['mag'].min())\n",
    "        a_mlk = np.log10(n) + b_mlk * node_df['mag'].min()\n",
    "                \n",
    "        # difference between the maximum observed and the maximum expected\n",
    "        diff_M_max_obs_exp = node_df['mag'].max() - a_lsq / b_lsq\n",
    "        \n",
    "        # mean and stdev time between mag 4.5 & 5 events\n",
    "        diffs = node_df.loc[(4.5 <= node_df['mag']) & (node_df['mag'] <= 5), 'time'].diff()\n",
    "\n",
    "        mu = diffs.mean().total_seconds()\n",
    "        c = diffs.std().total_seconds() / mu\n",
    "  \n",
    "        # Maximum magnitude in last seven days\n",
    "        # date_7days = node_df['time'].max() - pd.to_timedelta(7, unit='d')\n",
    "        # x_6i = node_df.loc[node_df['time'] > date_7days, 'mag'].max()\n",
    "\n",
    "        x_df = x_df.append(pd.DataFrame({\n",
    "            'clust_id': [node_id],\n",
    "            'sample_date': [sample_date],\n",
    "            'T_days': [T],\n",
    "            'M_mean': [M_mean],\n",
    "            'dEsq': [dEsq],\n",
    "            'a_lsq': [a_lsq],\n",
    "            'b_lsq': [b_lsq],\n",
    "            'a_mlk': [a_mlk],\n",
    "            'b_mlk': [b_mlk],\n",
    "            'mse':[mse],\n",
    "            'diff_M_max_obs_exp': [diff_M_max_obs_exp],\n",
    "            'mu': [mu],\n",
    "            'c': [c],\n",
    "            'quake_next30': [quake_next30],\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.groupby(['clust_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.groupby(['clust_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['clust_id']==node_id, 'time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df.loc[df['clust_id']==node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['clust_id']==node_id, 'mag'].hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
